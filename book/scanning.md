> 大口去吃。任何值得做的事情都值得玩命去做。
>
> <cite>Robert A. Heinlein, <em>Time Enough for Love</em></cite>

任何编译器或解释器的第一步都是<span name="lexing">scanning</span>(扫描)。扫描器以一系列字符的形式接收原始源代码，并将其分组为一系列我们称之为令牌( **tokens** )的块。这些是组成语言语法的有意义的“单词”和“标点”。

<aside name="lexing">

多年来，这项任务被不同地称为 "scanning" 和 "lexing" ( "lexical
analysis" 的简称)。很久以前，当计算机像Winnebagos房车一样大，但内存比你的手表还少时，一些人使用“scanner”仅指处理从磁盘读取原始源代码字符并将其缓冲在内存中的那段代码。然后“lexing”是对角色做有用的事情的后续阶段。

如今，将源文件读入内存是很平常的事情，所以它很少是编译器中的一个独特阶段。因此，这两个术语基本上可以互换。

</aside>
扫描对我们来说也是一个很好的起点，因为这段代码并不难--几乎就是一个夸大的 `switch` 语句。它可以帮助我们在以后处理一些更有趣的材料之前进行热身。在本章结束时，我们将拥有一个全功能的、快速的扫描器，它可以处理任何字符串的Lox源代码并产生令牌，我们将在下一章中把这些令牌送入分析器。

## 解释器框架

因为这是我们真正的第一章，在我们开始实际扫描一些代码之前，我们需要勾画出我们的解释器jlox的基本形状。一切都是从Java中的一个类开始的。

^code lox-class

<aside name="64">

对于退出代码，我使用UNIX中["sysexits.h"][sysexits]标头中定义的约定。这是我能找到的最接近标准的东西。

[sysexits]: https://www.freebsd.org/cgi/man.cgi?query=sysexits&amp;apropos=0&amp;sektion=0&amp;manpath=FreeBSD+4.3-RELEASE&amp;format=html

</aside>

把它放在一个文本文件中，然后去获取你的IDE或者Makefile或者其他设置。我在这里等你准备完毕,准备好了?出发!

Lox是一种脚本语言，这意味着它直接从源代码执行。我们的解释器支持两种运行代码的方式。如果从命令行启动jlox，并给它一个文件路径，它就会读取并执行该文件。

^code run-file

如果您想与解释器进行更亲密的对话，也可以交互运行它。在没有任何参数的情况下启动jlox，它会将您置于一个提示中，您可以在那里一次输入和执行一行代码。

<aside name="repl">

交互式提示也被称为“REPL”（发音类似于“rebel”，但带有“p”）。这个名字来自Lisp，在Lisp中，实现一个函数就像围绕几个内置函数包装一个循环一样简单：

```lisp
(print (eval (read)))
```

从最嵌套的调用向外工作，您读取( **R**ead )一行输入，对其求值(**E**valuate)，打印(**P**rint)结果，然后循环(**L**oop)并再次执行。

</aside>

^code prompt

 `readLine()` 函数，顾名思义，从命令行读取用户输入的一行内容，并返回结果。要终止一个交互式命令行应用程序，你通常需要键入 Control-D。这样做会向程序发出“文件结束”(EOF)的信号。当发生这种情况时， `readLine()` 返回 `null` ，所以我们检查此情况以退出循环。

提示符和文件运行器都是这个核心功能的简单包装:

^code run

由于我们还没有写出解释器，所以它还不是很有用，但不积跬步无以至千里。现在，它打印出我们即将实现的扫描器将发出的令牌，这样我们就可以看到我们是否取得了进展。

### 错误处理

在我们进行设置时，基础设施的另一个关键部分是*错误处理*。教科书有时会忽略这一点，因为它更像是一个实际问题而不是一个正式的计算机科学问题。但是，如果你想做一个真正*可用*的语言，那么优雅地处理错误是至关重要的。

我们的语言为处理错误所提供的工具构成了其用户界面的很大一部分。当用户的代码在工作时，他们根本没有想到我们的语言--他们的头脑中全是*他们的程序*。通常只有当事情出错时，他们才会注意到我们的实现。

<span name="errors">当</span>这种情况发生时，我们有责任向用户提供他们需要的所有信息，让他们了解哪里出错了，并引导他们慢慢回到他们想去的地方。做好这一点意味着从现在开始，在我们解释器的整个实现过程中考虑错误处理。

<aside name="errors">

说了这么多，对于*这个*解释器来说，我们要建立的是非常原始的东西。我很想谈谈交互式调试器、静态分析器和其他有趣的东西，但篇幅有限。

</aside>

^code lox-error

这个 `error()` 函数和它的 `report()` 助手告诉用户在给定的行中出现了一些语法错误。这是能够声称你*有*错误报告的底线。想象一下，如果您在某个函数调用中不小心留下了一个悬空逗号，解释器会打印出来:

```text
Error: Unexpected "," somewhere in your code. Good luck finding it!
```

这不是很有帮助。我们至少需要把他们指向正确的行。更好的做法是列出开头和结尾栏，这样他们就能知道这一行的位置。比这更好的是向用户*展示*错误的那行，比如：

```text
Error: Unexpected "," in argument list.

    15 | function(first, second,);
                               ^-- Here.
```

我很想在这本书中实现类似的东西，但老实说，这是一个非常繁琐的字符串操作代码。对用户来说非常有用，但在书中阅读并不是非常有趣，而且在技术上也不是很有趣。所以我们只保留一个行号。在你们自己的翻译中，请照我说的去做，不要照我做的去做。

我们把这个错误报告函数放在主Lox类中的主要原因是因为 `hadError` 字段。它的定义如下:

^code had-error (1 before)

我们将使用它来确保我们不会试图执行有已知错误的代码。此外，它让我们像一个好的命令行公民应该做的那样，用非零退出代码退出。

^code exit-code (1 before, 1 after)

我们需要在交互循环中重置此标志。如果用户犯了错误，它不应该终止他们的整个会话。

^code reset-had-error (1 before, 1 after)

我把错误报告放在这里而不是塞进扫描器和其他可能发生错误的阶段的另一个原因是为了提醒您，将*产生*错误的代码与*报告*错误的代码分开是一个很好的工程实践。

前端的各个阶段都会检测到错误，但是知道如何将错误呈现给用户并不是他们的工作。在全功能语言实现中，可能会有多种显示错误的方式:在 `stderr` 上，在IDE 的错误窗口中，记录到文件中，等等。您不希望这些代码遍布您的扫描器和解析器。

理想情况下，我们应该有一个实际的抽象，某种<span name="reporter">"ErrorReporter"</span>接口传递给扫描器和解析器，以便我们可以交换不同的报告策略。对于我们这里的简单解释器，我没有这样做，但我至少把错误报告的代码移到了不同的类中。

<aside name="reporter">

在我第一次实现jlox的时候，就是这么干的。我最后把它移除了，因为对于这本书中的最小解释器来说，它感觉设计得过于复杂了。

</aside>

有了一些基本的错误处理，我们的应用程序框架就准备好了。一旦我们有了一个带有 `scanTokens()` 方法的 Scanner 类，我们就可以开始运行它了。在这之前，让我们更精确地了解一下什么是令牌。


## Lexemes 和 Tokens

下面是一行Lox代码:

```lox
var language = "lox";
```

这里，`var` 是声明变量( `variable` )的关键字。那个三个字符的序列“v-a-r”意味着什么。但是如果我们从 `language` 中间抽出三个字母，比如“g-u-a”，它们本身没有任何意义。 

这就是词法分析的目的。我们的工作是扫描字符列表，并把它们组合成仍然代表某些东西的最小序列。这些字符块中的每一个都被称为一个 **lexeme** 。在示例代码行中，lexemes 是:

<img src="image/scanning/lexemes.png" alt="'var', 'language', '=', 'lox', ';'" />

Lexemes 只是源代码的原始子串。然而，在将字符序列组合成词位的过程中，我们也会偶然发现一些其他有用的信息。当我们把 lexemes 和其他数据捆绑在一起时，结果就是一个令牌(token)。它包括其他有用的东西，如:

### Token类型

关键字是语言语法的一部分，所以解析器通常会有这样的代码，“如果下一个 `token` 是 `while` ，那么执行... "。这意味着解析器不仅想知道它有某个标识符的`lexeme` ，还想知道它是否有 *保留* 关键字，以及有 *哪个* 关键字。 

<span name="ugly">解析器</span> 可以通过比较字符串来对原始 lexeme 中的 token 进行分类，但这很慢，而且有点难看。相反，在我们识别一个 lexeme 的时候，我们也会记住它代表 *哪种* lexeme 。我们为每个关键词、操作符、标点符号和字面类型都赋予不同的类型。

<aside name="ugly">

毕竟，字符串比较最终要看的是单个字符，而这不正是扫描器的工作吗？

</aside>

^code token-type

### 字面值

有用于字面值 lexeme 例如数字和字符串之类的。由于扫描器必须遍历文本中的每个字符以正确识别它，因此它还可以将该文本表示的值转换为解释器以后使用的活的运行时对象。

### 位置信息

早在我宣扬错误处理的福音时，我们就看到，我们需要告诉用户错误发生的地点。追踪这一点从这里开始。在我们简单的解释器中，我们只标记出在哪一行，但更复杂的实现也包括列和长度。

<aside name="location">

一些 token 实现将位置存储为两个数字：从源文件的开头到 lexeme 的开头的偏移量，以及 lexeme 的长度。扫描器需要知道这些，所以没有计算它们的开销。

偏移量可以通过回看源文件和计算前面的换行来转换为行和列的位置。这听起来很慢，而且确实如此。然而，*只有当你需要向用户实际显示行和列的时候* ，你才需要这样做。大多数 token 不会出现在错误信息中。对于这种情况，提前计算位置信息的时间越少越好。

</aside>

我们将所有这些数据打包成一个类。

^code token-class

现在我们有了一个可以用到对解释器的所有后期阶段的手脚架。

## 正则语言和表达式

现在我们知道了我们要生产的东西，让我们开工吧。扫描器的核心是一个循环。从源代码的第一个字符开始，扫描器找出该字符属于哪个 lexeme ，并消耗它和属于该 lexeme 的任何后续字符。当它到达该 lexeme 的末尾时，它发出一个 token 。

然后循环回去，从源代码中的下一个字符开始，再重复一遍。它一直这样做，吃字符，偶尔，呃，排泄出 token ，直到它到达输入的结尾。

<span name="alligator"></span>

<img src="image/scanning/lexigator.png" alt="An alligator eating characters and, well, you don't want to know." />

<aside name="alligator">

词法分析器.

</aside>

在循环的这一部分，我们查看一些字符，以确定它“匹配”哪种 lexeme ，这听起来可能很熟悉。如果您知道正则表达式，您可能会考虑为每种 lexeme 定义一个正则表达式，并使用它们来匹配字符。例如，对于标识符(变量名之类的)，Lox和C有相同的规则。可以用以下正则表达式匹配:

```text
[a-zA-Z_][a-zA-Z_0-9]*
```

如果你确实想到了正则表达式，你的直觉很准。决定一种特定语言如何将字符分组为 lexeme 的规则被称为其 <span name="theory">**lexical grammar**</span>。在Lox中，和大多数编程语言一样，该语法的规则简单到足以使该语言被归为 **[regular language][]** （正则语言）。这与正则表达式中的 "正则 "相同。

[regular language]: https://en.wikipedia.org/wiki/Regular_language

<aside name="theory">

我很痛心如此掩盖理论，尤其是当它像我认为的[Chomsky hierarchy][]（乔姆斯基分层）和[finite-state machines][]（有限状态自动机）那样有趣时。但老实说，其他书比我讲得更好。《编译原理 技术与工具》（[*Compilers: Principles, Techniques, and Tools*][dragon] 普遍被称为 "龙书"）是经典的参考书。

[chomsky hierarchy]: https://en.wikipedia.org/wiki/Chomsky_hierarchy
[dragon]: https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools
[finite-state machines]: https://en.wikipedia.org/wiki/Finite-state_machine

</aside>

如果你想的话，你 *可以* 使用正则表达式非常精确地识别 Lox 的所有不同的lexeme ，这背后有一堆有趣的理论，解释了为什么会这样，它意味着什么。像[Lex][]或[Flex][]这样的工具就是为了让你做到这一点而设计的——向它们扔几个正则表达式，它们就会<span name="lex">给</span>你一个完整的扫描器。

<aside name="lex">

Lex是由Mike Lesk和Eric Schmidt创建的。是的，就是那个担任谷歌执行主席的Eric Schmidt。我并不是说编程语言是通往财富和名声的必经之路，但我们中*至少*有一个巨型亿万富翁。

</aside>

[lex]: http://dinosaur.compilertools.net/lex/
[flex]: https://github.com/westes/flex

因为我们的目标是了解扫描仪是如何工作的，所以我们不会使用这类工具的。我们是纯手工的。

## 扫描类

事不宜迟，我们自己做个扫描类吧。

^code scanner-class

<aside name="static-import">

我知道静态导入被一些人认为是不好的风格，但它们使我不必在扫描器和分析器上到处撒下 `TokenType.` . 请原谅我，但书中的每个字符都很重要。

</aside>

我们将原始的源代码存储为一个简单的字符串，并且我们有一个准备好的列表，以填充我们要生成的 token 。前面提到的那个循环看起来像这样。

^code scan-tokens


扫描器遍历源代码，添加 token ，直到用完所有字符。然后，它追加一个最终的“文件结束”（EOF） token 。这并不是严格需要的，但它使我们的解析器更清晰。 

这个循环依赖于几个位置标记来跟踪扫描器在源代码中的位置。

^code scan-state (1 before, 2 after)

`start` 和 `current` 标记是索引到字符串中的偏移量。 `start` 指向被扫描的 lexeme 中的第一个字符， `current` 指向当前被考虑的字符。 `line` 跟踪当前所处的行，因此我们可以产生知道其位置的 token 。 

然后我们有一个小的帮助函数，告诉我们是否已经消耗了所有的字符。

^code is-at-end

## 识别 Lexemes

在每一轮循环中，我们扫描一个 token 。这是扫描器的真正核心。我们从简单的开始。想象一下，如果每个 lexeme 只有一个字符长。您需要做的就是使用下一个字符，并为它选择一个标记类型。有几个 lexeme 在Lox中*只是*单个字符，就从那些开始吧。

^code scan-token

<aside name="slash">

想知道为什么 `/` 没有在这里？别担心，我们会说到它的。

</aside>

同样，我们需要几个辅助函数。

^code advance-and-add-token

 `advance()` 方法消耗源文件中的下一个字符并返回它。其中 `advance()` 用于输入， `addToken()` 用于输出。它获取当前 lexeme 的文本，并为其创建一个新的 token 。我们将很快使用另一个重载来处理带有字面值的 token 。

### 词法错误

在我们走得太远之前，让我们花点时间考虑一下词法层面上的错误。如果用户向我们的解释器抛出一个包含 Lox 不使用的字符的源文件，如 `@#^` ，会发生什么？现在，这些字符被默默地丢弃了。 Lox 语言不使用这些字符，但这并不意味着解释器可以假装它们不存在。相反，我们会报告一个错误。

^code char-error (1 before, 1 after)

请注意，错误的字符仍然被先前对 `advance()` 的调用所消耗。这很重要，这样我们才不会陷入无限循环。 

还要注意，我们<span name="shotgun">*一直在扫描*</span>。程序后面可能还有其他错误。如果我们一次检测尽可能多的内容，会给我们的用户带来更好的体验。否则，他们看到一个微小的错误并修复它，结果却是下一个错误出现，如此等等。语法错误打地鼠一点也不好玩。 

(不用担心。由于 `hadError` 被设置，我们将永远不会尝试执行任何代码，即使我们继续前进并扫描其余部分。)

<aside name="shotgun">

代码分别报告每个无效字符，所以如果用户不小心粘贴了一大堆奇怪的文字，就会出现大量错误。将所有无效字符凝聚成一个错误会给用户带来更好的体验。

</aside>

### 操作符

单字符 lexeme 正常工作了，但这并不包括 Lox 的所有操作符。那么 `!` 呢？它是一个单字符，对吗？有时，是的，但是如果下一个字符是一个等号，那么我们应该创建一个 `!=` lexeme 。注意 `!` 和 `=` 不是两个独立的运算符。你不能在 Lox 中写成 `!   =` ，并让它表现得像一个不等式运算符。这就是为什么我们需要将 `!=` 作为一个 lexeme 来扫描。同样，`<` ,  `>` , 和 `=` 都可以在后面加上 `=` 来创建其他相等和比较运算符。

对于所有这些，我们需要看看第二个字符。

^code two-char-tokens (1 before, 2 after)

这种情况,我们可以用以下新函数处理:

^code match

这就像一个有条件的 `advance()` 。我们只消耗当前的字符，如果它是我们正在寻找的。

使用 `match()` ，我们分两个阶段识别这些 lexeme 。例如，当我们到达 `!` 时，我们跳到它的 `switch case` 。这意味着我们知道这个 lexeme 是以 `!` 开始的。然后，我们看下一个字符，以确定我们是在处理 `!=` 还是 `!` 。

## 更长的 Lexeme

我们仍然漏了一个操作符:  `/` ，除法。该字符需要一些特殊处理，因为注释也以斜杠开头。

^code slash (1 before, 2 after)

这与其他两个字符的运算符类似，只是当我们找到第二个 `/` 时，我们还没有结束这个符号。相反，我们继续消耗字符，直到到达行的末端。

这是我们处理较长 lexeme 的一般策略。当我们检测到一个词的开头时，我们会分流到一些特定词组的代码，这些代码会一直吃字符，直到它看到结尾。

我们还需要另一个帮手：

^code peek

它有点像 `advance()` ，但不消耗字符。这就是所谓的<span name="match">**lookahead**</span>（前瞻）。因为它只看当前未消耗的字符，所以我们有*一个字符的前瞻*。一般来说，这个数字越小，扫描器的运行速度就越快。词法的规则决定了我们需要多大的前瞻。幸运的是，大多数被广泛使用的语言只需要提前一到两个字符。

<aside name="match">

从技术上讲， `match()` 也是在做前瞻。  `advance()` 和 `peek()` 是基本运算符，而 `match()` 则是将它们结合起来。

</aside>

注释是 lexeme，但它们没有意义，解析器也不想处理它们。因此，当我们到达注释的末尾时，我们*不*调用 `addToken()` 。当我们回过头来开始下一个 lexeme 时，`start` 被重置，注释的 lexeme 烟消云散。

既然如此，现在是跳过那些其他无意义的字符的好时机：换行和空白。

^code whitespace (1 before, 3 after)


当遇到空白处时，我们只需回到扫描循环的起点。这将在空白字符之后开始一个新的 lexeme 。对于换行，我们做同样的事情，但我们也增加行计数器。(这就是为什么我们用 `peek()` 来寻找结束注释的换行，而不是用 `match()` 。我们希望那个换行符能把我们带到这里，这样我们就可以更新 `line` 了） 。

我们的扫描器越来越智能了。它可以处理相当自由的代码，如：

```lox
// this is a comment
(( )){} // grouping stuff
!*+-/=<> <= == // operators
```

### 字符串字面值

现在我们已经习惯了较长的 lexeme ，我们已经准备好处理文字。我们将首先处理字符串，因为它们总是以特定字符开头， `"` 。

^code string-start (1 before, 2 after)

这里将调用：

^code string

像注释一样，我们消耗字符，直到我们碰到结束字符串的 `"` 。我们也会优雅地处理在字符串关闭前输入完毕的情况，并为此报告一个错误。

没有特殊原因，Lox支持多行字符串。这有其利弊，但禁止它们比允许它们要复杂一点，所以我把它们放在一边。这意味着当我们在字符串中遇到换行符时，我们也需要更新  `line` 。

最后，最后一点有趣的是，当我们创建 token 时，我们还生成了实际的字符串*值*，解释器稍后将使用该值。在这里，这种转换只需要一个 `substring()` 来去除周围的引号。如果 Lox 支持像  `\n` 这样的转义序列，我们将在这里取消转义。

### 数字字面值

Lox中的所有数字在运行时都是浮点型的，但是支持整数和小数。数字字面值是一系列<span name="minus">数字</span>，可选地后跟一个 `.` 和一个或多个尾随数字。

<aside name="minus">

由于我们只寻找一个数字作为开始，这意味着 `-123` 不是一个数字字面值。相反，`-123` 是一个*表达式*，`-` 应用于数字字面值 `123` 。在实践中，结果是一样的，尽管它有一个有趣的边缘情况，如果我们要在数字上增加方法调用的话。请考虑一下:

```lox
print -123.abs();
```

这将打印出 `-123` ，因为取负值的优先级比方法调用低。我们可以通过使 `-` 成为数字字面值的一部分来解决这个问题。但是，再考虑一下:

```lox
var n = 123;
print -n.abs();
```

这仍然会产生 `-123` ，所以现在的语言似乎不一致了。不管你怎么做，有些情况最终会变得很奇怪。

</aside>

```lox
1234
12.34
```

我们不允许前导或尾随小数点，因此以下写法都是无效的：

```lox
.1234
1234.
```

我们可以很容易地支持前者，但为了保持简单，我把它排除在外。如果我们想允许`123.sqrt()` 这样的数字方法，后者就会变得很奇怪。

为了识别一个数字 lexeme 的开头，我们寻找任何数字。为每一个小数位添加 case 是很繁琐的，所以我们把它放在默认的 case 里。

^code digit-start (1 before, 1 after)

这依赖于这个小工具:

^code is-digit

<aside name="is-digit">

Java标准库提供了[`Character.isDigit()`][is-digit]，这似乎很合适。然而，该方法允许像梵文数字、全宽数字和其他我们不想要的有趣东西。

[is-digit]: http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#isDigit(char)

</aside>

一旦我们知道我们是处理一个数字，我们就分到一个单独的方法来消耗其余的字面量，就像我们对字符串做的那样。

^code number

我们在字面值的整数部分消耗尽可能多的数字。然后我们寻找小数部分，也就是小数点 (`.`) 后面至少有一个数字。如果我们确实有小数部分，同样，我们要尽可能多地消耗我们能找到的数字。

查看小数点后的数字需要第二个字符的提前量，因为我们不想在确定小数点后有一个数字之前消耗 `.` 。所以我们添加:

^code peek-next

<aside name="peek-next">

我本可以让 `peek()` 接受一个参数来决定要看多少个字符，而不是定义两个函数，但这将允许任意提前量。提供这两个函数可以使代码的读者更清楚地了解到我们的扫描器最多只能向前看两个字符。

</aside>

最后，我们将 lexeme 转换为其数字值。我们的解释器使用 Java 的 `Double` 类型来表示数字，所以我们制造了一个该类型的值。我们使用 Java 自带的解析方法将 lexeme 转换为真正的 Java double 类型。我们可以自己实现这一点，但是，说实话，除非你想为即将到来的技术面试做准备，否则不值得花时间。

剩下的字面值是布尔值和  `nil` ，但是我们将把它们当作关键字来处理，这样我们就可以......

## 保留字和标识符

我们的扫描器就要完成了。词法中剩下要实现的部分只有标识符和他们的表亲，保留字。您可能认为我们可以用处理多字符操作符（如 `<=` ）的方法来匹配 `or`  等关键字。

```java
case 'o':
  if (match('r')) {
    addToken(OR);
  }
  break;
```

考虑一下如果用户将变量命名为 `orchid` 会发生什么。扫描器将看到前两个字母  `or` ，并立即发出  `or` 关键字标记。这就引出了一个重要的原则，叫做<span name="maximal">**maximal munch**（贪婪匹配）</span>。当两个词法规则都能匹配扫描器正在查看的代码块时，匹配字符最多的规则获胜。

That rule states that if we can match `orchid` as an identifier and `or` as a
keyword, then the former wins. This is also why we tacitly assumed, previously,
that `<=` should be scanned as a single `<=` token and not `<` followed by `=`.

该规则规定，如果我们可以匹配 `orchid` 作为标识符和 `or` 作为关键字，则前者获胜。这也是为什么我们先前默认地假设 `<=` 应该作为单个 `<=` token 而不是 `<` 后面跟着 `=` 来扫描。


<aside name="maximal">

看看这段令人生厌的c代码:

```c
---a;
```

它有效么？这取决于扫描器如何拆分单词，如果扫描器这么拆分：

```c
- --a;
```

这样可以解析。但这需要扫描器了解周围代码的语法结构，这会使事情变得比我们想要的更复杂。相反，贪婪匹配规则总是扫描成：


```c
-- -a;
```

它以这种方式扫描，即使会导致解析器稍后出现语法错误。

</aside>

贪婪匹配意味着我们无法轻易检测到一个保留字，直到我们到达了一个标识符的末尾。毕竟，保留字是一个标识符，它只是一个被语言声明为内部使用的词。这就是 **reserved word** 保留字这个词的由来。

因此，我们开始假设任何以字母或下划线开头的 lexeme 都是标识符。

^code identifier-start (3 before, 3 after)

代码的其余部分位于此处：

^code identifier

我们用这些辅助函数：

^code is-alpha

这使标识符发挥作用。为了处理关键字，我们查看标识符的 lexeme 是否是保留词之一。如果是，我们使用特定于该关键字的 token 类型。我们在 map 中定义了一组保留字。

^code keyword-map

然后，我们扫描到一个标识符后，检查它是否与 map 中的任何内容匹配。

^code keyword-type (2 before, 1 after)

如果匹配,我们就是用该关键字的 token 类型。否则，他就是一个普通的用户定义的标识符。

就这样，我们现在有了一个完整的Lox词法扫描器。启动 REPL ，输入一些有效和无效的代码。它是否产生您期望的 token ？试着想出一些有趣的边缘案例 看看它是否能够处理它们。

<div class="challenges">

## 挑战

1.  Python 和 Haskell 的词法是非 *常规* 的。这是什么意思，为什么它们不是呢？

1.  除了分隔 token 例如:--区分 `print foo` 和 `printfoo` -- 空格在大多数语言中没有什么用处。然而，在一些黑暗的角落里，空格 *确实* 影响了 CoffeeScript 、Ruby和C语言预处理器对代码的解析。在这些语言中，它在哪里以及有什么影响呢？

1.  我们这里的扫描器和大多数扫描器一样，抛弃了注释和空白，因为分析器不需要这些东西。什么时候你会想写一个*不*丢弃这些东西的扫描器？它有什么用处？

1.  为 Lox 的扫描器增加对 C 风格  `/* ... */`  块注释的支持。确保能处理其中的换行符。考虑允许它们嵌套。增加对嵌套的支持的工作量是否比你预期的要多？为什么？

</div>

<div class="design-note">

## 设计须知: 隐式分号

今天的程序员在语言方面被宠坏了，对语法也变得挑剔起来。他们希望自己的语言看起来干净、现代。几乎每一种新的语言都会刮掉一点语法上的地皮（有些古老的语言如BASIC从来没有过），那就是 `;` 作为一个明确的语句终止符。

相反，在有意义的地方，他们将换行作为语句的结束符。"有意义的地方 "这部分是具有挑战性的。虽然大多数语句都在自己的行上，但有时你需要将一条语句分散到几行上。这些混杂的换行线不应该被当作终止符来处理。

大多数应该忽略换行的明显情况都很容易发现，但也有一些讨厌的情况。

* 返回值在下一行中:

    ```js
    if (condition) return
    "value"
    ```

    "value" 是被返回的值，还是我们有一个没有值的 `return` 语句，后面是一个包含字符串字面值的表达式语句？

* 括号表达式在下一行中:

    ```js
    func
    (parenthesized)
    ```

    这是一个对  `func(parenthesized)` 函数的调用,还是两个表达式,一个是 `func` ,另一个是括号表达式?

* A `-` 在新行中:

    ```js
    first
    -second
    ```

    Is this `first - second` -- an infix subtraction -- or two expression
    statements, one for `first` and one to negate `second`?
    这 `first - second` 是中缀减法还是两个表达式语句，一个 `first`，另一个 `-second`？

在所有这些案例中，无论是将换行符作为分隔符还是不作为分隔符都会产生有效的代码，但可能不是用户想要的代码。在不同的语言中，有各种令人不安的规则用来决定哪些换行符是分隔符。这里有几个例子。

*   [Lua][] 完全无视换行符，但仔细控制其语法，在大多数情况下，语句之间根本就不需要分隔符。这完全是合法的:

    ```lua
    a = 1 b = 2
    ```

    Lua通过要求 `return` 语句是一个块中的最后一个语句来避免 `return` 问题。如果在关键字 `end` 之前的 `return` 后面有一个值，它必须是用于`return` 的。对于其他两种情况，他们允许使用明确的 `;` ，并期望用户使用它。在实践中，这几乎从未发生过，因为括号或单数否定表达式的语句没有任何意义。


*   [Go][] 在扫描器中处理换行。如果换行出现在已知可能结束语句的少数标记类型之后，换行会被当作分号处理。否则，它将被忽略。Go团队提供了一个规范的代码格式化器，[gofmt][] ，而且生态系统热衷于使用它，这确保了成语风格的代码在这个简单的规则下运行良好。


*   [Python][] 将所有的换行线都视为有效，除非在行末使用明确的反斜杠将其延续到下一行。然而，在一对大括号 (`()`, `[]`、或 `{}`) 内的任何地方的换行线都被忽略。惯用风格更倾向于后者。

    这个规则对 Python 很适用，因为它是一种高度面向语句的语言。特别是， Python 的语法确保语句不会出现在表达式中。 C 也是这样做的，但许多其他具有“lambda”或函数字面量语法的语言却没有。

    JavaScript中的一个例子：

    ```js
    console.log(function() {
      statement();
    });
    ```

    此处，`console.log()` *表达式* 包含一个函数字面量，该函数字面量又包含 *语句* `statement();` 。

    如果一个<span name="lambda">语句</span>中，换行符应该变得有意义，同时仍然嵌套在括号内，那么 Python 将需要一套不同的隐含连接行的规则。

<aside name="lambda">

现在你知道为什么Python的 `lambda` 只允许一个表达式体了吧。

</aside>

*   JavaScript 的 "自动插入分号 "规则("[automatic semicolon insertion][asi]")才是真正奇怪的。其他语言认为大多数换行符都是有意义的，在多行语句中只有少数换行符应该被忽略，而JS的假设恰恰相反。它把所有的换行线都视为无意义的空白，除非它遇到了一个解析错误。如果遇到了，它就会回去尝试把前一个换行线变成分号，以获得语法上有效的东西。

    如果我对这个问题的工作原理进行详细的说明，那么这篇设计说明就会变成一篇设计参劾了，更不用说JavaScript的 "解决方案 "是各种坏主意的集合了。这是个混乱的问题。JavaScript是我所知道的唯一一种语言，在这种语言中，许多风格指南都要求在每个语句后面都有明确的分号，尽管这种语言理论上允许你省略它们。

如果你正在设计一种新的语言，你几乎肯定应该避免使用明确的语句终止符。程序员和其他人类一样都是时尚的创造者，分号就像所有关键字必须大写一样已经过时了。确保你选择一套对你的语言的特定语法和习惯有意义的规则就没问题，而且不要像JavaScript那样做。


</div>

[lua]: https://www.lua.org/pil/1.1.html
[go]: https://golang.org/ref/spec#Semicolons
[gofmt]: https://golang.org/cmd/gofmt/
[python]: https://docs.python.org/3.5/reference/lexical_analysis.html#implicit-line-joining
[asi]: https://www.ecma-international.org/ecma-262/5.1/#sec-7.9
